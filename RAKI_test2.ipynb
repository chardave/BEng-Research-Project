{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chardave/BEng-Research-Project/blob/main/RAKI_test2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4jWgo1p1sZ0",
        "outputId": "027f15c9-f9c8-40fb-9e86-b072c9d82dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#mount to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import print_function\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers, optimizers\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "#from tensorflow.examples.tutorials.mnist import input_data\n",
        "import time\n",
        "import os\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "zZD3_Msj3QXA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Filepaths for the Excel files\n",
        "k_sampled_file = \"/content/drive/MyDrive/MEng group project/imagescan_data_test2.xlsx\"\n",
        "ref_scan_file = \"/content/drive/MyDrive/MEng group project/refscan_data_test2.xlsx\"\n",
        "\n",
        "# Number of coils and slices\n",
        "num_coils = 4\n",
        "num_slices = 4\n",
        "\n",
        "# Function to load and reshape complex data\n",
        "def load_complex_data_3d(file, prefix):\n",
        "    # Initialize a dictionary to store 3D matrices for each slice\n",
        "    slice_data = {}\n",
        "\n",
        "    for slice_idx in range(1, num_slices + 1):\n",
        "        # Create a 3D matrix for this slice (x, y, coil)\n",
        "        slice_matrix = []\n",
        "\n",
        "        for coil in range(1, num_coils + 1):\n",
        "            # Construct sheet names\n",
        "            real_sheet = f\"real_{prefix}_c{coil}_s{slice_idx}\"\n",
        "            imag_sheet = f\"imag_{prefix}_c{coil}_s{slice_idx}\"\n",
        "\n",
        "            # Read the real and imaginary parts\n",
        "            real_part = pd.read_excel(file, sheet_name=real_sheet, header=None).to_numpy()\n",
        "            imag_part = pd.read_excel(file, sheet_name=imag_sheet, header=None).to_numpy()\n",
        "\n",
        "            # Combine into a complex matrix and append to the slice\n",
        "            coil_matrix = real_part + 1j * imag_part\n",
        "            slice_matrix.append(coil_matrix)\n",
        "\n",
        "        # Stack along the third dimension (coil axis)\n",
        "        slice_data[f\"slice_{slice_idx}\"] = np.stack(slice_matrix, axis=-1)\n",
        "\n",
        "    return slice_data\n",
        "\n",
        "# Load k-sampled and reference data into 3D matrices\n",
        "k_sampled_slices = load_complex_data_3d(k_sampled_file, \"imagedat\")\n",
        "ref_scan_slices = load_complex_data_3d(ref_scan_file, \"refdat\")\n",
        "\n",
        "# Example access: k_sampled_slices[\"slice_1\"] or ref_scan_slices[\"slice_4\"]\n",
        "# Printing the shape of one of the loaded slices\n",
        "print(f\"Shape of k-sampled slice 1: {k_sampled_slices['slice_1'].shape}\")\n",
        "print(f\"Shape of ref-scan slice 4: {ref_scan_slices['slice_4'].shape}\")\n",
        "\n",
        "# Output: Each key corresponds to a 192x133x4 complex matrix."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDkCUeke9N17",
        "outputId": "dacae36a-272b-4b5f-95d2-db41d0661a4b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of k-sampled slice 1: (192, 133, 4)\n",
            "Shape of ref-scan slice 4: (128, 50, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define functions to get the weight and bias and convolution"
      ],
      "metadata": {
        "id": "UO4PHTAPIznM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# weight initialisation function\n",
        "def weight_variable(shape,vari_name):\n",
        "    initial = tf.random.normal(shape, mean=0.0, stddev=0.1,dtype=tf.float32)\n",
        "    return tf.Variable(initial,name = vari_name)\n",
        "    # neural network weights need to start with random values before training,\n",
        "    # which helps in breaking symmetry and ensuring the model learns meanigful features\n",
        "\n",
        "# bias initialisation function\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape,dtype=tf.float32)\n",
        "    return tf.Variable(initial)\n",
        "    # used to shift the output of the neurons and provide more flexibility to the model\n",
        "    # initialising bias with a small positive value helps avod issues like 'dying RELU'\n",
        "    # where all neurons are stuck in the zero state due to negative biases\n",
        "\n",
        "# standard 2D convolution function\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='VALID')\n",
        "    # strides defines how te filter moves across the input tensor - moves by 1 in both height\n",
        "    # width dimensions with no padding in the batch and channel dimensions\n",
        "    # padding = 'Valid' - output size will be smaller than the input size\n",
        "\n",
        "# dilated 2D convolution function\n",
        "def conv2d_dilate(x, W,dilate_rate):\n",
        "    return tf.nn.convolution(x, W, dilations = [1,dilate_rate])\n",
        "    # increases receptive field of the convolutional filter without\n",
        "    # increasing the number of parameters\n"
      ],
      "metadata": {
        "id": "P6JOxYpF2ua9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the learning function\n",
        "def learning(ACS_input, target_input, accrate_input):\n",
        "    # Define input dimensions based on the inputs\n",
        "    ACS_dim_X, ACS_dim_Y, ACS_dim_Z = ACS_input.shape[1:]\n",
        "    target_dim_X, target_dim_Y, target_dim_Z = target_input.shape[1:]\n",
        "    # Ensure target and ACS have matching shapes\n",
        "    # by cropping/padding as necessary.\n",
        "\n",
        "    diff_X = ACS_dim_X - target_dim_X\n",
        "    diff_Y = ACS_dim_Y - target_dim_Y\n",
        "\n",
        "    # Handle X dimension\n",
        "    if diff_X > 0:  # ACS is larger, crop ACS\n",
        "        start_X = diff_X // 2\n",
        "        end_X = start_X + target_dim_X\n",
        "        ACS_input = ACS_input[:, start_X:end_X, :, :]\n",
        "    elif diff_X < 0:  # Target is larger, pad ACS\n",
        "        pad_X = abs(diff_X) // 2\n",
        "        ACS_input = np.pad(ACS_input, ((0, 0), (pad_X, pad_X), (0, 0), (0, 0)), mode='constant')\n",
        "\n",
        "    # Handle Y dimension\n",
        "    if diff_Y > 0:  # ACS is larger, crop ACS\n",
        "        start_Y = diff_Y // 2\n",
        "        end_Y = start_Y + target_dim_Y\n",
        "        ACS_input = ACS_input[:, :, start_Y:end_Y, :]\n",
        "    elif diff_Y < 0:  # Target is larger, pad ACS\n",
        "        pad_Y = abs(diff_Y) // 2\n",
        "        ACS_input = np.pad(ACS_input, ((0, 0), (0, 0), (pad_Y, pad_Y), (0, 0)), mode='constant')\n",
        "\n",
        "    # Update dimensions after cropping/padding\n",
        "    ACS_dim_X, ACS_dim_Y, ACS_dim_Z = ACS_input.shape[1:]\n",
        "\n",
        "    # Build the model\n",
        "    inputs = tf.keras.Input(shape=(ACS_dim_X, ACS_dim_Y, ACS_dim_Z))\n",
        "\n",
        "    # Convolutional Layers\n",
        "    conv1 = layers.Conv2D(\n",
        "        filters=layer1_channels,\n",
        "        kernel_size=(kernel_x_1, kernel_y_1),\n",
        "        activation='relu',\n",
        "        padding='same',\n",
        "        dilation_rate=(1, accrate_input)\n",
        "    )(inputs)\n",
        "\n",
        "    conv2 = layers.Conv2D(\n",
        "        filters=layer2_channels,\n",
        "        kernel_size=(kernel_x_2, kernel_y_2),\n",
        "        activation='relu',\n",
        "        padding='same',\n",
        "        dilation_rate=(1, accrate_input)\n",
        "    )(conv1)\n",
        "\n",
        "    conv3 = layers.Conv2D(\n",
        "        filters=target_dim_Z,\n",
        "        kernel_size=(kernel_last_x, kernel_last_y),\n",
        "        activation=None,\n",
        "        padding='same',\n",
        "        dilation_rate=(1, accrate_input)\n",
        "    )(conv2)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model(inputs=inputs, outputs=conv3)\n",
        "\n",
        "    # Compile the model\n",
        "    optimizer = optimizers.Adam(learning_rate=LearningRate)\n",
        "    model.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        x=ACS_input,\n",
        "        y=target_input,\n",
        "        batch_size=1,\n",
        "        epochs=MaxIteration,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Get the trained weights\n",
        "    W_conv1, W_conv2, W_conv3 = [layer.get_weights()[0] for layer in model.layers if 'conv2d' in layer.name]\n",
        "\n",
        "    # Final error (loss) after training\n",
        "    error = history.history['loss'][-1]\n",
        "\n",
        "    return [W_conv1, W_conv2, W_conv3, error]"
      ],
      "metadata": {
        "id": "__3FN5CmPZmN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the network architecture"
      ],
      "metadata": {
        "id": "jt_45WpxPr4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_3layer(input_kspace,w1,b1,w2,b2,w3,b3,acc_rate):\n",
        "    h_conv1 = tf.nn.relu(conv2d_dilate(input_kspace, w1,acc_rate))\n",
        "    h_conv2 = tf.nn.relu(conv2d_dilate(h_conv1, w2,acc_rate))\n",
        "    h_conv3 = conv2d_dilate(h_conv2, w3,acc_rate)\n",
        "    return h_conv3"
      ],
      "metadata": {
        "id": "VsDO_EgmPnKZ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###################### Reconstruction Parameters ######################\n",
        "\n",
        "#### Network Parameters ####\n",
        "kernel_x_1 = 3\n",
        "kernel_y_1 = 3\n",
        "\n",
        "kernel_x_2 = 3\n",
        "kernel_y_2 = 3\n",
        "\n",
        "kernel_last_x = 1\n",
        "kernel_last_y = 1\n",
        "\n",
        "layer1_channels = 16\n",
        "layer2_channels = 8\n",
        "\n",
        "MaxIteration = 1\n",
        "LearningRate = 1e-3\n",
        "\n",
        "#### Input/Output Data ####\n",
        "#inputData = 'rawdata.mat'\n",
        "#input_variable_name = 'kspace'\n",
        "slices = list(k_sampled_slices.keys()) # list of slice keys\n",
        "resultName = 'RAKI_recon'\n",
        "recon_variable_name = 'kspace_recon'\n",
        "\n",
        "######################################################################\n",
        "\n",
        "for slice_name in slices:\n",
        "  print(f\"Processing {slice_name}...\")\n",
        "\n",
        "  # Extract the slice data\n",
        "  kspace = k_sampled_slices[slice_name]\n",
        "\n",
        "\n",
        "  # normalise the k-space data\n",
        "  normalise = 0.015/np.max(abs(kspace[:]))\n",
        "  kspace = np.multiply(kspace,normalise)\n",
        "  #ACS = np.multiply(ACS, normalise)\n",
        "\n",
        "  # save dimension of the kspace data and number of coils -\n",
        "  # should give m1=192, n1=133 and no_ch=4\n",
        "  [m1, n1, no_ch] = np.shape(kspace)\n",
        "  no_inds = 1\n",
        "  kspace_all = kspace;\n",
        "  kx = np.transpose(np.int32([(range(1,m1+1))]))\n",
        "  ky = np.int32([(range(1,n1+1))])\n",
        "\n",
        "  # create a mask to identify columns in the k-space data that have non-zero values\n",
        "  kspace = np.copy(kspace_all)\n",
        "  mask = np.squeeze(np.sum(np.sum(np.abs(kspace),0),1))>0;\n",
        "  picks = np.where(mask == 1);\n",
        "  kspace = kspace[:,np.int32(picks[0][0]):n1+1,:]\n",
        "  kspace_all = kspace_all[:,np.int32(picks[0][0]):n1+1,:]\n",
        "\n",
        "  # preserve a copy of the original data\n",
        "  kspace_NEVER_TOUCH = np.copy(kspace_all)\n",
        "\n",
        "  # WHAT IS HAPPENING HERE\n",
        "  mask = np.squeeze(np.sum(np.sum(np.abs(kspace),0),1))>0;\n",
        "  picks = np.where(mask == 1);\n",
        "  d_picks = np.diff(picks,1)\n",
        "  indic = np.where(d_picks == 1);\n",
        "\n",
        "  # I STILL DONT KNOW WHAT IS HAPPENING HERE\n",
        "  mask_x = np.squeeze(np.sum(np.sum(np.abs(kspace),2),1))>0;\n",
        "  picks_x = np.where(mask_x == 1);\n",
        "  x_start = picks_x[0][0]\n",
        "  x_end = picks_x[0][-1]\n",
        "\n",
        "  # now process the ACS/Refscan\n",
        "  ACS = ref_scan_slices[slice_name]\n",
        "  [ACS_dim_X, ACS_dim_Y, ACS_dim_Z] = np.shape(ACS)\n",
        "  ACS_re = np.zeros([ACS_dim_X,ACS_dim_Y,ACS_dim_Z*2])\n",
        "  ACS_re[:,:,0:no_ch] = np.real(ACS)\n",
        "  ACS_re[:,:,no_ch:no_ch*2] = np.imag(ACS)\n",
        "\n",
        "  acc_rate = d_picks[0][0]\n",
        "  no_channels = ACS_dim_Z*2\n",
        "\n",
        "  # save kernel and network parameters in a .mat file\n",
        "  name_weight = resultName + ('_weight_%d%d,%d%d,%d%d_%d,%d.mat' % (kernel_x_1,kernel_y_1,kernel_x_2,kernel_y_2,kernel_last_x,kernel_last_y,layer1_channels,layer2_channels))\n",
        "  name_image = resultName + ('_image_%d%d,%d%d,%d%d_%d,%d.mat' % (kernel_x_1,kernel_y_1,kernel_x_2,kernel_y_2,kernel_last_x,kernel_last_y,layer1_channels,layer2_channels))\n",
        "\n",
        "  existFlag = os.path.isfile(name_image)\n",
        "\n",
        "  # initialises 3D convolutional kernels - [1, 2, 3, 4, 5] - 1 and 2 are spatial size kernel_x, kernel_y, 3 is input channels, 4 is output channels for layers 1 and 2,\n",
        "  # 5 maintains input channels for multicoil data - number of output valies for reconstruction\n",
        "\n",
        "  # first layer\n",
        "  w1_all = np.zeros([kernel_x_1, kernel_y_1, no_channels, layer1_channels, no_channels],dtype=np.float32)\n",
        "  # second layer\n",
        "  w2_all = np.zeros([kernel_x_2, kernel_y_2, layer1_channels,layer2_channels,no_channels],dtype=np.float32)\n",
        "  # third and last layer\n",
        "  w3_all = np.zeros([kernel_last_x, kernel_last_y, layer2_channels,acc_rate - 1, no_channels],dtype=np.float32)\n",
        "\n",
        "  # bias initialisation - flags control to include biases in the layers\n",
        "  b1_flag = 0\n",
        "  b2_flag = 0\n",
        "  b3_flag = 0\n",
        "\n",
        "  if (b1_flag == 1):\n",
        "      b1_all = np.zeros([1,1, layer1_channels,no_channels]);\n",
        "  else:\n",
        "      b1 = []\n",
        "\n",
        "  if (b2_flag == 1):\n",
        "      b2_all = np.zeros([1,1, layer2_channels,no_channels])\n",
        "  else:\n",
        "      b2 = []\n",
        "\n",
        "  if (b3_flag == 1):\n",
        "      b3_all = np.zeros([1,1, layer3_channels, no_channels])\n",
        "  else:\n",
        "      b3 = []\n",
        "\n",
        "  #target region in ACS for learning and reconstruction\n",
        "  target_x_start = np.int32((np.ceil(kernel_x_1/2)-1) + (np.ceil(kernel_x_2/2)-1) + (np.ceil(kernel_last_x/2)-1)) * acc_rate\n",
        "  target_x_end = np.int32(ACS_dim_X - target_x_start -1)\n",
        "\n",
        "  time_ALL_start = time.time()\n",
        "\n",
        "  [ACS_dim_X, ACS_dim_Y, ACS_dim_Z] = np.shape(ACS_re)\n",
        "  ACS = np.reshape(ACS_re, [1,ACS_dim_X, ACS_dim_Y, ACS_dim_Z])\n",
        "  ACS = np.float32(ACS)\n",
        "\n",
        "  target_y_start = np.int32((np.ceil(kernel_y_1/2)-1) + (np.ceil(kernel_y_2/2)-1) + (np.ceil(kernel_last_y/2)-1)) * acc_rate\n",
        "  target_y_end = ACS_dim_Y  - np.int32((np.floor(kernel_y_1/2) + (np.floor(kernel_y_2/2)) + np.floor(kernel_last_y/2))) * acc_rate -1\n",
        "\n",
        "  target_dim_X = target_x_end - target_x_start + 1\n",
        "  target_dim_Y = target_y_end - target_y_start + 1\n",
        "\n",
        "  target_dim_Z = acc_rate - 1\n",
        "\n",
        "  ## TESTING ##\n",
        "\n",
        "  print('go!')\n",
        "  time_Learn_start = time.time()\n",
        "\n",
        "  errorSum = 0\n",
        "\n",
        "  if tf.compat.v1.get_default_session():\n",
        "    tf.compat.v1.keras.backend.clear_session()\n",
        "  config = tf.compat.v1.ConfigProto()\n",
        "\n",
        "  for ind_c in range (ACS_dim_Z):\n",
        "    # process each channel of the ACS data separately\n",
        "    # initialises a tensorflow session with specified configurations\n",
        "      # sess = tf.compat.v1.Session(config=config) - dont need to do this\n",
        "      # set target lines\n",
        "      target = np.zeros([1,target_dim_X,target_dim_Y,target_dim_Z]) # target_dim_Z depth corresponds to acc_rate-1\n",
        "                                                                    # number of interpolation targets between sampled points\n",
        "                                                                    # outputs array filled with relevant ACS data for each undersampled slice\n",
        "      print('learning channel #',ind_c+1)\n",
        "      time_channel_start = time.time()\n",
        "\n",
        "      # loops through each undersampled slice in the y-direction to define the part of the target that corresponds to that acceleration index\n",
        "      for ind_acc in range(acc_rate-1):\n",
        "          # adds/subtracts contributions from the kernel sizes to avoid boundary issues and multiplies by acc_rate to align with the undersampling pattern\n",
        "          target_y_start = np.int32((np.ceil(kernel_y_1/2)-1) + (np.ceil(kernel_y_2/2)-1) + (np.ceil(kernel_last_y/2)-1)) * acc_rate + ind_acc + 1\n",
        "          target_y_end = ACS_dim_Y  - np.int32((np.floor(kernel_y_1/2) + (np.floor(kernel_y_2/2)) + np.floor(kernel_last_y/2))) * acc_rate + ind_acc\n",
        "          # assigns the corresponding region of th ACS for the current slice to the target array\n",
        "          target[0,:,:,ind_acc] = ACS[0,target_x_start:target_x_end + 1, target_y_start:target_y_end +1,ind_c];\n",
        "          #print(target.shape)\n",
        "          #print(ACS.shape)\n",
        "\n",
        "      # learning - outputs trained convolutional kernels for each layer\n",
        "      [w1,w2,w3,error] = learning(ACS,target,acc_rate)\n",
        "\n",
        "      w1_all[:,:,:,:,ind_c] = w1\n",
        "      w2_all[:,:,:,:,ind_c] = w2\n",
        "      w3_all[:,:,:,:,ind_c] = w3\n",
        "      time_channel_end = time.time()\n",
        "      print('Time cost: ', time_channel_end-time_channel_start,'s')\n",
        "      print('Norm of Error = ',error)\n",
        "      errorSum = errorSum + error\n",
        "\n",
        "  time_Learn_end = time.time()\n",
        "  print('learning step costs: ', (time_Learn_end - time_Learn_start)/60, 'min')\n",
        "\n",
        "  sio.savemat(name_weight, {'w1': w1_all, 'w2': w2_all, 'w3': w3_all})\n",
        "\n",
        "  # initialise copies of full k-space data to store reconstructed k-space data\n",
        "  kspace_recon_all = np.copy(kspace_all)\n",
        "  kspace_recon_all_nocenter = np.copy(kspace_all)\n",
        "\n",
        "  kspace = np.copy(kspace_all)\n",
        "\n",
        "  # identify undersampled k-space indices based on acceleration rate\n",
        "  # use setdiff1d to find which indices in picks are not covered by the undersampling pattern\n",
        "  over_samp = np.setdiff1d(picks,np.int32([range(0, n1,acc_rate)]))\n",
        "  kspace_und = kspace\n",
        "  kspace_und[:,over_samp,:]=0\n",
        "  [dim_kspaceUnd_X,dim_kspaceUnd_Y,dim_kspaceUnd_Z] = np.shape(kspace_und)\n",
        "\n",
        "  # prepare the k-space for neural network input\n",
        "  # neural networks require real-valued input so real and imaginary parts are separated\n",
        "  kspace_und_re = np.zeros([dim_kspaceUnd_X, dim_kspaceUnd_Y, dim_kspaceUnd_Z*2])\n",
        "  kspace_und_re[:,:,0:dim_kspaceUnd_Z] = np.real(kspace_und)\n",
        "  kspace_und_re[:,:,dim_kspaceUnd_Z:dim_kspaceUnd_Z*2] = np.imag(kspace_und)\n",
        "\n",
        "  # reshape tensor to 4D - 1st dimension is batch size (1)\n",
        "  kspace_und_re = np.float32(kspace_und_re)\n",
        "  kspace_und_re = np.reshape(kspace_und_re,[1,dim_kspaceUnd_X,dim_kspaceUnd_Y,dim_kspaceUnd_Z*2])\n",
        "  kspace_recon = kspace_und_re\n",
        "\n",
        "  ## start reconstruction\n",
        "\n",
        "  if tf.compat.v1.get_default_session():\n",
        "    tf.compat.v1.keras.backend.clear_session()\n",
        "  config = tf.compat.v1.ConfigProto()\n",
        "  config.gpu_options.per_process_gpu_memory_fraction = 1/3 ;\n",
        "\n",
        "  for ind_c in range(0, no_channels):\n",
        "    print('Reconstructing Channel #',ind_c+1)\n",
        "\n",
        "    #sess = tf.compat.v1.Session(config=config)\n",
        "    #if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:\n",
        "    #    init = tf.initialize_all_variables()\n",
        "    #else:\n",
        "    #  init = tf.compat.v1.global_variables_initializer()\n",
        "    #sess.run(init)\n",
        "\n",
        "    # grab w and b\n",
        "    w1 = np.float32(w1_all[:,:,:,:,ind_c])\n",
        "    w2 = np.float32(w2_all[:,:,:,:,ind_c])\n",
        "    w3 = np.float32(w3_all[:,:,:,:,ind_c])\n",
        "\n",
        "    if (b1_flag == 1):\n",
        "        b1 = b1_all[:,:,:,ind_c]\n",
        "    if (b2_flag == 1):\n",
        "        b2 = b2_all[:,:,:,ind_c]\n",
        "    if (b3_flag == 1):\n",
        "        b3 = b3_all[:,:,:,ind_c]\n",
        "\n",
        "    # perform 3-layer convolutional reconstruction\n",
        "    res = cnn_3layer(kspace_und_re,w1,b1,w2,b2,w3,b3,acc_rate)\n",
        "\n",
        "    # assign reconstructed data to kspace_recon\n",
        "    #target_x_end_kspace = dim_kspaceUnd_X - target_x_start\n",
        "    #target_x_end_kspace = target_x_end - target_x_start + 1  # Change calculation\n",
        "    target_x_end_kspace = target_x_start + res[0,:,::acc_rate,ind_acc].shape[0]\n",
        "    print(target_x_end_kspace)\n",
        "    # reconstruct k-space data by filling in the missing values (undersampled)\n",
        "    # with the reconstructed values (res)\n",
        "\n",
        "    #for ind_acc in range(0,acc_rate-1):\n",
        "\n",
        "     #   target_y_start = np.int32((np.ceil(kernel_y_1/2)-1) + np.int32((np.ceil(kernel_y_2/2)-1)) + np.int32(np.ceil(kernel_last_y/2)-1)) * acc_rate + ind_acc + 1;\n",
        "     #   target_y_end_kspace = dim_kspaceUnd_Y - np.int32((np.floor(kernel_y_1/2)) + (np.floor(kernel_y_2/2)) + np.floor(kernel_last_y/2)) * acc_rate + ind_acc;\n",
        "     #   kspace_recon[0,target_x_start:target_x_end_kspace,target_y_start:target_y_end_kspace+1:acc_rate,ind_c] = res[0,:,::acc_rate,ind_acc]\n",
        "\n",
        "    for ind_acc in range(0,acc_rate-1):\n",
        "\n",
        "      target_y_start = np.int32((np.ceil(kernel_y_1/2)-1) + np.int32((np.ceil(kernel_y_2/2)-1)) + np.int32(np.ceil(kernel_last_y/2)-1)) * acc_rate + ind_acc + 1;\n",
        "      target_y_end_kspace = dim_kspaceUnd_Y - np.int32((np.floor(kernel_y_1/2)) + (np.floor(kernel_y_2/2)) + np.floor(kernel_last_y/2)) * acc_rate + ind_acc;\n",
        "\n",
        "      # Get the shape of the target slice\n",
        "      target_shape = kspace_recon[0,target_x_start:target_x_end_kspace,target_y_start:target_y_end_kspace+1:acc_rate,ind_c].shape\n",
        "\n",
        "      # Reshape or slice the 'res' array to match the target shape\n",
        "      res_slice = res[0,:,::acc_rate,ind_acc][:target_shape[0], :target_shape[1]]\n",
        "\n",
        "      kspace_recon[0,target_x_start:target_x_end_kspace,target_y_start:target_y_end_kspace+1:acc_rate,ind_c] = res_slice\n",
        "\n",
        "  # remove batch dimension\n",
        "  kspace_recon = np.squeeze(kspace_recon)\n",
        "\n",
        "  # combine real and imaginary parts to reconstruct complex k-space data\n",
        "  kspace_recon_complex = (kspace_recon[:,:,0:np.int32(no_channels/2)] + np.multiply(kspace_recon[:,:,np.int32(no_channels/2):no_channels],1j))\n",
        "  kspace_recon_all_nocenter[:,:,:] = np.copy(kspace_recon_complex);\n",
        "\n",
        "  for sli in range(0,no_ch):\n",
        "      kspace_recon_all[:,:,sli] = np.fft.ifft2(kspace_recon_all[:,:,sli])\n",
        "\n",
        "  rssq = (np.sum(np.abs(kspace_recon_all)**2,2)**(0.5))\n",
        "  sio.savemat(name_image,{recon_variable_name:kspace_recon_complex})\n",
        "\n",
        "  time_ALL_end = time.time()\n",
        "  print('All process costs ',(time_ALL_end-time_ALL_start)/60,'mins')\n",
        "  print('Error Average in Training is ',errorSum/no_channels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "l-IFiBOROswC",
        "outputId": "2472cf9b-d7e3-4cff-8b03-138cede0c7ca"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing slice_1...\n",
            "go!\n",
            "learning channel # 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 884ms/step - loss: 2.9308e-10\n",
            "Time cost:  0.9602491855621338 s\n",
            "Norm of Error =  2.9307586912885597e-10\n",
            "learning channel # 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 886ms/step - loss: 9.8179e-11\n",
            "Time cost:  0.9608359336853027 s\n",
            "Norm of Error =  9.817913149134938e-11\n",
            "learning channel # 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 858ms/step - loss: 1.0829e-10\n",
            "Time cost:  0.9328868389129639 s\n",
            "Norm of Error =  1.0828673374652098e-10\n",
            "learning channel # 4\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 3.7321e-10\n",
            "Time cost:  1.0995700359344482 s\n",
            "Norm of Error =  3.732148201596175e-10\n",
            "learning channel # 5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 2.6097e-10\n",
            "Time cost:  1.3278372287750244 s\n",
            "Norm of Error =  2.609736038383659e-10\n",
            "learning channel # 6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.3360e-10\n",
            "Time cost:  1.3049888610839844 s\n",
            "Norm of Error =  1.3359988115801968e-10\n",
            "learning channel # 7\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.5052e-10\n",
            "Time cost:  1.4058260917663574 s\n",
            "Norm of Error =  1.5051841717461656e-10\n",
            "learning channel # 8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 854ms/step - loss: 4.4616e-10\n",
            "Time cost:  0.9256801605224609 s\n",
            "Norm of Error =  4.4616227379279394e-10\n",
            "learning step costs:  0.14888500372568766 min\n",
            "Reconstructing Channel # 1\n",
            "198\n",
            "Reconstructing Channel # 2\n",
            "198\n",
            "Reconstructing Channel # 3\n",
            "198\n",
            "Reconstructing Channel # 4\n",
            "198\n",
            "Reconstructing Channel # 5\n",
            "198\n",
            "Reconstructing Channel # 6\n",
            "198\n",
            "Reconstructing Channel # 7\n",
            "198\n",
            "Reconstructing Channel # 8\n",
            "198\n",
            "All process costs  0.15148301919301352 mins\n",
            "Error Average in Training is  2.330013413112675e-10\n",
            "Processing slice_2...\n",
            "go!\n",
            "learning channel # 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 875ms/step - loss: 3.0958e-10\n",
            "Time cost:  0.9526665210723877 s\n",
            "Norm of Error =  3.095814160580801e-10\n",
            "learning channel # 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 841ms/step - loss: 1.0702e-10\n",
            "Time cost:  0.9075484275817871 s\n",
            "Norm of Error =  1.0701658309519857e-10\n",
            "learning channel # 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844ms/step - loss: 8.5019e-11\n",
            "Time cost:  0.9231202602386475 s\n",
            "Norm of Error =  8.501882531319893e-11\n",
            "learning channel # 4\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 921ms/step - loss: 3.8222e-10\n",
            "Time cost:  0.9889578819274902 s\n",
            "Norm of Error =  3.8222358611506024e-10\n",
            "learning channel # 5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 895ms/step - loss: 2.7966e-10\n",
            "Time cost:  0.9601829051971436 s\n",
            "Norm of Error =  2.796551878958553e-10\n",
            "learning channel # 6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 873ms/step - loss: 1.0536e-10\n",
            "Time cost:  0.9392998218536377 s\n",
            "Norm of Error =  1.0535756295171339e-10\n",
            "learning channel # 7\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844ms/step - loss: 1.0394e-10\n",
            "Time cost:  0.9101426601409912 s\n",
            "Norm of Error =  1.0394244492895055e-10\n",
            "learning channel # 8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 881ms/step - loss: 4.6350e-10\n",
            "Time cost:  0.960820198059082 s\n",
            "Norm of Error =  4.6350301374786795e-10\n",
            "learning step costs:  0.12592333555221558 min\n",
            "Reconstructing Channel # 1\n",
            "198\n",
            "Reconstructing Channel # 2\n",
            "198\n",
            "Reconstructing Channel # 3\n",
            "198\n",
            "Reconstructing Channel # 4\n",
            "198\n",
            "Reconstructing Channel # 5\n",
            "198\n",
            "Reconstructing Channel # 6\n",
            "198\n",
            "Reconstructing Channel # 7\n",
            "198\n",
            "Reconstructing Channel # 8\n",
            "198\n",
            "All process costs  0.12871139446894328 mins\n",
            "Error Average in Training is  2.2953732751324063e-10\n",
            "Processing slice_3...\n",
            "go!\n",
            "learning channel # 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 844ms/step - loss: 2.4076e-10\n",
            "Time cost:  0.9151322841644287 s\n",
            "Norm of Error =  2.407570531826053e-10\n",
            "learning channel # 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 9.1951e-11\n",
            "Time cost:  1.2492897510528564 s\n",
            "Norm of Error =  9.195062766753637e-11\n",
            "learning channel # 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.1663e-10\n",
            "Time cost:  1.5028908252716064 s\n",
            "Norm of Error =  1.166307050937121e-10\n",
            "learning channel # 4\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 3.5631e-10\n",
            "Time cost:  1.5246477127075195 s\n",
            "Norm of Error =  3.5630573491651774e-10\n",
            "learning channel # 5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 845ms/step - loss: 2.6559e-10\n",
            "Time cost:  0.9129848480224609 s\n",
            "Norm of Error =  2.655910491533575e-10\n",
            "learning channel # 6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - loss: 1.2375e-10\n",
            "Time cost:  2.9376299381256104 s\n",
            "Norm of Error =  1.237466379366836e-10\n",
            "learning channel # 7\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 929ms/step - loss: 9.5614e-11\n",
            "Time cost:  1.001201868057251 s\n",
            "Norm of Error =  9.561353792042482e-11\n",
            "learning channel # 8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 892ms/step - loss: 3.9007e-10\n",
            "Time cost:  0.9610908031463623 s\n",
            "Norm of Error =  3.9006620156101235e-10\n",
            "learning step costs:  0.18374821345011394 min\n",
            "Reconstructing Channel # 1\n",
            "198\n",
            "Reconstructing Channel # 2\n",
            "198\n",
            "Reconstructing Channel # 3\n",
            "198\n",
            "Reconstructing Channel # 4\n",
            "198\n",
            "Reconstructing Channel # 5\n",
            "198\n",
            "Reconstructing Channel # 6\n",
            "198\n",
            "Reconstructing Channel # 7\n",
            "198\n",
            "Reconstructing Channel # 8\n",
            "198\n",
            "All process costs  0.1863285700480143 mins\n",
            "Error Average in Training is  2.1008269342898123e-10\n",
            "Processing slice_4...\n",
            "go!\n",
            "learning channel # 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 890ms/step - loss: 2.4828e-10\n",
            "Time cost:  0.9713180065155029 s\n",
            "Norm of Error =  2.48282477910422e-10\n",
            "learning channel # 2\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893ms/step - loss: 7.2837e-11\n",
            "Time cost:  0.9716699123382568 s\n",
            "Norm of Error =  7.283700032001406e-11\n",
            "learning channel # 3\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 897ms/step - loss: 8.5473e-11\n",
            "Time cost:  0.9711875915527344 s\n",
            "Norm of Error =  8.547259428004494e-11\n",
            "learning channel # 4\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 879ms/step - loss: 3.5287e-10\n",
            "Time cost:  0.9470160007476807 s\n",
            "Norm of Error =  3.52872980835528e-10\n",
            "learning channel # 5\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 2.4563e-10\n",
            "Time cost:  1.335313081741333 s\n",
            "Norm of Error =  2.4562984979326075e-10\n",
            "learning channel # 6\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.2887e-10\n",
            "Time cost:  1.4103598594665527 s\n",
            "Norm of Error =  1.2887009515072378e-10\n",
            "learning channel # 7\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.2868e-10\n",
            "Time cost:  1.446251392364502 s\n",
            "Norm of Error =  1.286792339350029e-10\n",
            "learning channel # 8\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 956ms/step - loss: 4.1686e-10\n",
            "Time cost:  1.0659904479980469 s\n",
            "Norm of Error =  4.1686096241555504e-10\n",
            "learning step costs:  0.1522725224494934 min\n",
            "Reconstructing Channel # 1\n",
            "198\n",
            "Reconstructing Channel # 2\n",
            "198\n",
            "Reconstructing Channel # 3\n",
            "198\n",
            "Reconstructing Channel # 4\n",
            "198\n",
            "Reconstructing Channel # 5\n",
            "198\n",
            "Reconstructing Channel # 6\n",
            "198\n",
            "Reconstructing Channel # 7\n",
            "198\n",
            "Reconstructing Channel # 8\n",
            "198\n",
            "All process costs  0.15494134028752646 mins\n",
            "Error Average in Training is  2.0993814933006893e-10\n"
          ]
        }
      ]
    }
  ]
}